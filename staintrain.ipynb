{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d49f777-ddc6-4c8f-8065-0552ff8249bf",
   "metadata": {},
   "source": [
    "# Stain Detection Model Training\n",
    "\n",
    "June 21, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41723a8d-0ce0-4129-ac11-aa3029eb5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install required packages\"\"\"\n",
    "    packages = [\n",
    "        'ultralytics',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'onnx',\n",
    "        'onnxruntime-gpu',\n",
    "        'opencv-python',\n",
    "        'pillow',\n",
    "        'matplotlib',\n",
    "        'seaborn',\n",
    "        'pandas',\n",
    "        'ipynbname'\n",
    "    ]\n",
    "    \n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *packages])\n",
    "    \n",
    "    print(\"All packages installed successfully!\")\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8e237-0acc-4ae0-9370-6386a05db909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU check \n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"=== GPU Check ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b50196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import ipynbname\n",
    "\n",
    "def split_dataset_for_yolo(source_dir, dest_dir, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets.\n",
    "\n",
    "    :param source_dir: Directory containing the original dataset.\n",
    "    :param train_dir: Directory to save the training set.\n",
    "    :param val_dir: Directory to save the validation set.\n",
    "    :param split_ratio: Ratio of training data to total data (default is 0.8).\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove the output dirs recursively and unconditionally to ensure they do not exist.\n",
    "\n",
    "   \n",
    "    # Create the image and label dirs\n",
    "    for d in ['train','test','val']:\n",
    "        this_dir = os.path.join(dest_dir,d)\n",
    "        if os.path.exists(this_dir):\n",
    "            shutil.rmtree(this_dir)\n",
    "        os.makedirs(this_dir)\n",
    "        os.makedirs(os.path.join(this_dir, 'images'))\n",
    "        os.makedirs(os.path.join(this_dir, 'labels'))\n",
    "\n",
    "    # Get all files in the source directory\n",
    "    all_files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f)) and f.endswith('.jpg')]\n",
    "    \n",
    "    # Shuffle the files\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(all_files) * split_ratio)\n",
    "\n",
    "    # Split the files into training and validation sets\n",
    "    train_files = all_files[:split_index]\n",
    "    val_files = all_files[split_index:]\n",
    "\n",
    "    # Select 10% random files for test set\n",
    "    random.shuffle(all_files)\n",
    "    split_index = int(len(all_files) * 0.1)\n",
    "    test_files = all_files[:split_index]\n",
    "\n",
    "    # Move files to their respective directories\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, \"train\", \"images\", file))\n",
    "        label_file = os.path.splitext(file)[0] + '.txt'\n",
    "        if os.path.exists(os.path.join(source_dir, label_file)):\n",
    "            shutil.copy(os.path.join(source_dir, label_file), os.path.join(dest_dir, \"train\", \"labels\", label_file))\n",
    "\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, \"val\", \"images\", file))\n",
    "        label_file = os.path.splitext(file)[0] + '.txt'\n",
    "        if os.path.exists(os.path.join(source_dir, label_file)):\n",
    "            shutil.copy(os.path.join(source_dir, label_file), os.path.join(dest_dir, \"val\", \"labels\", label_file))\n",
    "    \n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, \"test\", \"images\", file))\n",
    "        label_file = os.path.splitext(file)[0] + '.txt'\n",
    "        if os.path.exists(os.path.join(source_dir, label_file)):\n",
    "            shutil.copy(os.path.join(source_dir, label_file), os.path.join(dest_dir, \"test\", \"labels\", label_file))\n",
    "    \n",
    "    print(f\"Dataset split completed: {len(train_files)} training files, {len(test_files)} test files, {len(val_files)} validation files.\")\n",
    "\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # try as Notebook\n",
    "    notebook_path = ipynbname.path()\n",
    "    script_dir = os.path.dirname(notebook_path)\n",
    "\n",
    "print(\"Script directory:\", script_dir)\n",
    "\n",
    "ds_directory = os.path.join(script_dir, \"dataset\")\n",
    "source_directory = os.path.join(ds_directory,\"src\")\n",
    "\n",
    "split_dataset_for_yolo(source_directory, ds_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885684d-1abc-47c6-8a82-28301e8813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation with Contrast Enhancement\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def apply_contrast_enhancement(image, method='clahe', **kwargs):\n",
    "    \"\"\"Apply various contrast enhancement techniques\"\"\"\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        # Convert to grayscale for processing, then back to BGR\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image.copy()\n",
    "    \n",
    "    if method == 'clahe':\n",
    "        clip_limit = kwargs.get('clip_limit', random.uniform(1.5, 3.0))\n",
    "        tile_size = kwargs.get('tile_size', random.choice([8, 16]))\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(tile_size, tile_size))\n",
    "        enhanced = clahe.apply(gray)\n",
    "        \n",
    "    elif method == 'histogram_eq':\n",
    "        enhanced = cv2.equalizeHist(gray)\n",
    "        \n",
    "    elif method == 'contrast_stretch':\n",
    "        lower_pct = kwargs.get('lower_pct', random.uniform(1, 3))\n",
    "        upper_pct = kwargs.get('upper_pct', random.uniform(97, 99))\n",
    "        lower = np.percentile(gray, lower_pct)\n",
    "        upper = np.percentile(gray, upper_pct)\n",
    "        enhanced = np.clip((gray - lower) / (upper - lower) * 255, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    elif method == 'adaptive_gamma':\n",
    "        # Adaptive gamma correction based on image statistics\n",
    "        mean_intensity = np.mean(gray)\n",
    "        gamma = 2.2 if mean_intensity > 127 else 0.8\n",
    "        gamma += random.uniform(-0.2, 0.2)  # Add randomness\n",
    "        \n",
    "        # Create lookup table\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "        enhanced = cv2.LUT(gray, table)\n",
    "    \n",
    "    # Convert back to BGR if original was color\n",
    "    if len(image.shape) == 3:\n",
    "        enhanced = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    return enhanced\n",
    "\n",
    "def augment_dataset(source_dir, work_dir, augmentation_factor=3):\n",
    "    \"\"\"\n",
    "    Create augmented dataset with contrast enhancement and other augmentations\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Original dataset directory (dataset/src)\n",
    "        work_dir: Working directory for augmented data (dataset/work)\n",
    "        augmentation_factor: How many augmented versions per original image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clear and create work directory structure\n",
    "    work_path = Path(work_dir)\n",
    "    if work_path.exists():\n",
    "        shutil.rmtree(work_path)\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for subdir in ['images', 'labels']:\n",
    "            (work_path / split / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    source_path = Path(source_dir)\n",
    "    image_files = list(source_path.glob('*.jpg')) + list(source_path.glob('*.jpeg')) + list(source_path.glob('*.png'))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to augment\")\n",
    "    \n",
    "    # Split files\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    # Use your existing split ratios\n",
    "    train_split = int(len(image_files) * 0.8)\n",
    "    test_split = int(len(image_files) * 0.1)\n",
    "    \n",
    "    splits = {\n",
    "        'train': image_files[:train_split],\n",
    "        'test': image_files[train_split:train_split + test_split],\n",
    "        'val': image_files[train_split + test_split:]\n",
    "    }\n",
    "    \n",
    "    total_processed = 0\n",
    "    \n",
    "    for split_name, files in splits.items():\n",
    "        print(f\"Processing {split_name} split: {len(files)} files\")\n",
    "        \n",
    "        for img_file in files:\n",
    "            # Copy original files first\n",
    "            img_name = img_file.name\n",
    "            label_name = img_file.stem + '.txt'\n",
    "            label_file = source_path / label_name\n",
    "            \n",
    "            # Original image and label\n",
    "            shutil.copy2(img_file, work_path / split_name / 'images' / img_name)\n",
    "            if label_file.exists():\n",
    "                shutil.copy2(label_file, work_path / split_name / 'labels' / label_name)\n",
    "            \n",
    "            # Create augmented versions\n",
    "            if split_name == 'train':  # Only augment training data\n",
    "                img = cv2.imread(str(img_file))\n",
    "                \n",
    "                for aug_idx in range(augmentation_factor):\n",
    "                    # Random contrast enhancement method\n",
    "                    method = random.choice(['clahe', 'contrast_stretch', 'adaptive_gamma'])\n",
    "                    enhanced_img = apply_contrast_enhancement(img, method=method)\n",
    "                    \n",
    "                    # Additional random augmentations\n",
    "                    augmented_img = enhanced_img.copy()\n",
    "                    \n",
    "                    # Random brightness adjustment\n",
    "                    if random.random() < 0.5:\n",
    "                        brightness = random.randint(-30, 30)\n",
    "                        augmented_img = cv2.convertScaleAbs(augmented_img, alpha=1, beta=brightness)\n",
    "                    \n",
    "                    # Random noise (very subtle for fabric)\n",
    "                    if random.random() < 0.3:\n",
    "                        noise = np.random.normal(0, random.uniform(1, 3), augmented_img.shape).astype(np.int16)\n",
    "                        augmented_img = np.clip(augmented_img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Random slight blur (to simulate camera focus variations)\n",
    "                    if random.random() < 0.2:\n",
    "                        kernel_size = random.choice([3, 5])\n",
    "                        augmented_img = cv2.GaussianBlur(augmented_img, (kernel_size, kernel_size), 0)\n",
    "                    \n",
    "                    # Save augmented image\n",
    "                    aug_img_name = f\"{img_file.stem}_aug{aug_idx}{img_file.suffix}\"\n",
    "                    aug_label_name = f\"{img_file.stem}_aug{aug_idx}.txt\"\n",
    "                    \n",
    "                    cv2.imwrite(str(work_path / split_name / 'images' / aug_img_name), augmented_img)\n",
    "                    \n",
    "                    # Copy label file for augmented image\n",
    "                    if label_file.exists():\n",
    "                        shutil.copy2(label_file, work_path / split_name / 'labels' / aug_label_name)\n",
    "            \n",
    "            total_processed += 1\n",
    "            if total_processed % 10 == 0:\n",
    "                print(f\"Processed {total_processed}/{len(image_files)} images\")\n",
    "    \n",
    "    # Count final dataset\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        img_count = len(list((work_path / split / 'images').glob('*')))\n",
    "        print(f\"{split}: {img_count} images\")\n",
    "    \n",
    "    print(f\"✅ Augmented dataset created in {work_dir}\")\n",
    "    return work_dir\n",
    "\n",
    "# Create augmented dataset\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    notebook_path = ipynbname.path()\n",
    "    script_dir = os.path.dirname(notebook_path)\n",
    "\n",
    "source_directory = os.path.join(script_dir, \"dataset\", \"src\")\n",
    "work_directory = os.path.join(script_dir, \"dataset\", \"work\")\n",
    "\n",
    "if os.path.exists(work_directory):\n",
    "    shutil.rmtree(work_directory)\n",
    "os.makedirs(work_directory, exist_ok=True)\n",
    "\n",
    "print(\"Creating augmented dataset...\")\n",
    "augmented_dataset_dir = augment_dataset(source_directory, work_directory, augmentation_factor=3)\n",
    "\n",
    "# Create data.yaml pointing to augmented dataset\n",
    "data_yaml_content = f\"\"\"\n",
    "# Stain Detection Dataset Configuration\n",
    "path: {os.path.join(script_dir, \"dataset\", \"work\")}  # dataset root dir\n",
    "train: train/images  # train images (relative to 'path')\n",
    "val: val/images      # val images (relative to 'path')\n",
    "test: test/images    # test images (relative to 'path')\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: stain\n",
    "\n",
    "# Number of classes\n",
    "nc: 1\n",
    "\"\"\"\n",
    "\n",
    "# Write the data.yaml file\n",
    "with open('data.yaml', 'w') as f:\n",
    "    f.write(data_yaml_content.strip())\n",
    "\n",
    "print(\"✅ data.yaml updated to use augmented dataset\")\n",
    "print(f\"Dataset path: {os.path.join(script_dir, 'dataset', 'work')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d432711e-4a88-4a79-b3b0-999e8be83f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset structure validation\n",
    "\n",
    "def validate_dataset_structure():\n",
    "    \"\"\"Validate that the dataset structure is correct\"\"\"\n",
    "    \n",
    "    # Check for data.yaml\n",
    "    if not os.path.exists('data.yaml'):\n",
    "        print(\"❌ data.yaml not found!\")\n",
    "        return False\n",
    "    \n",
    "    # Load and display data.yaml\n",
    "    with open('data.yaml', 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"=== Dataset Configuration ===\")\n",
    "    for key, value in data_config.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Check directories\n",
    "    required_dirs = ['train', 'val', 'test']  # Note: 'val' not 'valid' in some configs\n",
    "    \n",
    "    for split in required_dirs:\n",
    "        img_dir = data_config.get(split, f'../{split}/images')\n",
    "        # Handle relative paths\n",
    "        if img_dir.startswith('../'):\n",
    "            img_dir = img_dir[3:]  # Remove '../'\n",
    "\n",
    "        img_dir = os.path.join(data_config['path'], img_dir)\n",
    "        if os.path.exists(img_dir):\n",
    "            img_count = len([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            print(f\"✅ {split}: {img_count} images found in {img_dir}\")\n",
    "        else:\n",
    "            print(f\"❌ {split} directory not found: {img_dir}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Validate dataset\n",
    "dataset_valid = validate_dataset_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecda5e-0460-44df-b5a2-389d5539d0c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test system\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def check_memory_and_disk():\n",
    "    \"\"\"Check current memory and disk usage\"\"\"\n",
    "    \n",
    "    # Check RAM\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"RAM: {memory.used/1024**3:.1f}GB used / {memory.total/1024**3:.1f}GB total ({memory.percent:.1f}%)\")\n",
    "    \n",
    "    # Check disk space\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk: {disk.used/1024**3:.1f}GB used / {disk.total/1024**3:.1f}GB total ({disk.used/disk.total*100:.1f}%)\")\n",
    "    \n",
    "    # Check GPU memory if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU: {gpu_memory:.1f}GB used / {gpu_total:.1f}GB total\")\n",
    "    \n",
    "    # Check shared memory\n",
    "    try:\n",
    "        shm_stats = os.statvfs('/dev/shm')\n",
    "        shm_total = shm_stats.f_frsize * shm_stats.f_blocks / 1024**3\n",
    "        shm_free = shm_stats.f_frsize * shm_stats.f_bavail / 1024**3\n",
    "        print(f\"Shared Memory: {shm_total-shm_free:.1f}GB used / {shm_total:.1f}GB total\")\n",
    "    except:\n",
    "        print(\"Shared memory stats not available\")\n",
    "\n",
    "def clean_memory():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Memory cleaned\")\n",
    "\n",
    "def clear_temp_files():\n",
    "    \"\"\"Clear PyTorch temporary files\"\"\"\n",
    "    import tempfile\n",
    "    import glob\n",
    "    \n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    torch_files = glob.glob(os.path.join(temp_dir, 'torch_*'))\n",
    "    \n",
    "    removed_count = 0\n",
    "    for file_path in torch_files:\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "                removed_count += 1\n",
    "        except Exception as e:\n",
    "            pass  # Ignore files in use\n",
    "    \n",
    "    print(f\"Cleared {removed_count} temporary PyTorch files\")\n",
    "\n",
    "check_memory_and_disk()\n",
    "clear_temp_files()\n",
    "clean_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35eee3a-dd08-49ec-aab4-2ab0fa421ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "\n",
    "import random\n",
    "\n",
    "def visualize_sample_data(num_samples=6):\n",
    "    \"\"\"Visualize sample images with their labels\"\"\"\n",
    "    \n",
    "    # Find sample images\n",
    "    with open('data.yaml', 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "        train_dir = os.path.join(data_config['path'], data_config['train'])\n",
    "    if not os.path.exists(train_dir):\n",
    "        train_dir = './train/images'\n",
    "    \n",
    "    if not os.path.exists(train_dir):\n",
    "        print(\"❌ Train directory not found!\")\n",
    "        return\n",
    "    \n",
    "    image_files = [f for f in os.listdir(train_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"❌ No images found in train directory!\")\n",
    "        return\n",
    "    \n",
    "    # Plot samples\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(num_samples, len(image_files))):\n",
    "        img_path = os.path.join(train_dir, random.choice(image_files))\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load corresponding label\n",
    "        label_path = img_path.replace('/images/', '/labels/').replace('.jpg', '.txt')\n",
    "        \n",
    "        axes[i].imshow(img_rgb)\n",
    "        axes[i].set_title(f'Sample {i+1}: {image_files[i]}')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Try to load and display bounding box if label exists\n",
    "        if os.path.exists(label_path):\n",
    "            try:\n",
    "                with open(label_path, 'r') as f:\n",
    "                    label_data = f.read().strip()\n",
    "                    if label_data:\n",
    "                        # Parse YOLO format: class x_center y_center width height (normalized)\n",
    "                        parts = label_data.split()\n",
    "                        if len(parts) >= 5:\n",
    "                            _, x_center, y_center, width, height = map(float, parts[:5])\n",
    "                            \n",
    "                            # Convert to pixel coordinates\n",
    "                            img_h, img_w = img_rgb.shape[:2]\n",
    "                            x1 = int((x_center - width/2) * img_w)\n",
    "                            y1 = int((y_center - height/2) * img_h)\n",
    "                            x2 = int((x_center + width/2) * img_w)\n",
    "                            y2 = int((y_center + height/2) * img_h)\n",
    "                            \n",
    "                            # Draw bounding box\n",
    "                            from matplotlib.patches import Rectangle\n",
    "                            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, \n",
    "                                           edgecolor='red', facecolor='none')\n",
    "                            axes[i].add_patch(rect)\n",
    "                            axes[i].set_title(f'Sample {i+1}: {image_files[i]} (with stain)')\n",
    "            except Exception as e:\n",
    "                print(f\"Could not parse label for {image_files[i]}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "if dataset_valid:\n",
    "    visualize_sample_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541729c7-de21-4384-bc37-c2de2246248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss implementation for object detection\n",
    "    \n",
    "    This addresses class imbalance by down-weighting easy examples\n",
    "    and focusing training on hard examples (like subtle stains)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Convert predictions to probabilities\n",
    "        p = torch.sigmoid(pred)\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "        \n",
    "        # Calculate p_t\n",
    "        p_t = p * target + (1 - p) * (1 - target)\n",
    "        \n",
    "        # Calculate alpha_t\n",
    "        alpha_t = self.alpha * target + (1 - self.alpha) * (1 - target)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class EnhancedTrainingConfig:\n",
    "    def __init__(self):\n",
    "        # Model settings\n",
    "        self.model_size = 'yolo11n.pt'  # Start with nano\n",
    "        \n",
    "        # Training parameters - ENHANCED FOR SUBTLE DETECTION\n",
    "        self.epochs = 150  # Increased for better convergence\n",
    "        self.batch_size = 8\n",
    "        self.imgsz = 640\n",
    "        self.patience = 30  # Increased patience\n",
    "        \n",
    "        # Paths\n",
    "        self.data_yaml = 'data.yaml'\n",
    "        self.project_name = 'fabric_stain_enhanced'\n",
    "        self.experiment_name = 'stain_detection_focal_v1'\n",
    "        \n",
    "        # Hardware\n",
    "        self.device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "        self.workers = 0\n",
    "        \n",
    "        # Memory optimization\n",
    "        self.cache = False\n",
    "        self.amp = True\n",
    "        self.save = True\n",
    "        self.verbose = True\n",
    "        \n",
    "        # CRITICAL: Enhanced loss function parameters\n",
    "        self.cls_loss_weight = 1.0  # Standard classification weight\n",
    "        self.box_loss_weight = 7.5  # Increased box regression weight\n",
    "        self.dfl_loss_weight = 1.5  # Distribution focal loss weight\n",
    "        \n",
    "        # Confidence and NMS thresholds - CRITICAL FOR FALSE POSITIVES\n",
    "        self.conf_threshold = 0.25  # Lower initial confidence\n",
    "        self.iou_threshold = 0.7    # Higher IoU for NMS\n",
    "        \n",
    "        # Optimizer settings - tuned for subtle features\n",
    "        self.optimizer = 'AdamW'  # Better for fine-tuning\n",
    "        self.lr0 = 0.001  # Lower learning rate for stability\n",
    "        self.lrf = 0.001  # Lower final learning rate\n",
    "        self.momentum = 0.937\n",
    "        self.weight_decay = 0.0005\n",
    "        self.single_cls = True\n",
    "        \n",
    "        # Enhanced augmentation for fabric textures\n",
    "        self.hsv_h = 0.008    # Reduced hue variation (fabric color consistency)\n",
    "        self.hsv_s = 0.3      # Reduced saturation (white fabric)\n",
    "        self.hsv_v = 0.2      # Reduced brightness variation\n",
    "        self.degrees = 5.0    # Reduced rotation (fabric typically flat)\n",
    "        self.translate = 0.1  # Reduced translation\n",
    "        self.scale = 0.2      # Reduced scaling\n",
    "        self.fliplr = 0.5     # Keep horizontal flip\n",
    "        self.flipud = 0.1     # Minimal vertical flip\n",
    "        self.perspective = 0.0002  # Minimal perspective change\n",
    "        self.mixup = 0.0      # Disable mixup (can blur stain boundaries)\n",
    "        self.copy_paste = 0.0 # Disable copy-paste\n",
    "        \n",
    "        # Early stopping and validation settings\n",
    "        self.val_period = 1   # Validate every epoch\n",
    "        self.save_period = 10 # Save checkpoint every 10 epochs\n",
    "        \n",
    "        # Class balancing (for handling class imbalance)\n",
    "        self.cls_pw = 1.0     # Positive weight for classification\n",
    "        self.obj_pw = 1.0     # Positive weight for objectness\n",
    "        \n",
    "    def display(self):\n",
    "        \"\"\"Display configuration\"\"\"\n",
    "        print(\"=== Enhanced Training Configuration ==\")\n",
    "        for attr, value in self.__dict__.items():\n",
    "            print(f\"{attr}: {value}\")\n",
    "\n",
    "# Create enhanced config\n",
    "config = EnhancedTrainingConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fad4f-cf0f-4ef0-b42b-dc0818a0d1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "def train_yolo_model(config):\n",
    "    \"\"\"Train the YOLO model with enhanced settings for subtle feature detection\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting YOLOv11 training for stain detection...\")\n",
    "    print(f\"Using device: {config.device}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(config.model_size)\n",
    "    \n",
    "    # Start training with enhanced parameters\n",
    "    results = model.train(\n",
    "        data=config.data_yaml,\n",
    "        epochs=config.epochs,\n",
    "        imgsz=config.imgsz,\n",
    "        batch=config.batch_size,\n",
    "        patience=config.patience,\n",
    "        save=config.save,\n",
    "        cache=config.cache,\n",
    "        device=config.device,\n",
    "        workers=config.workers,\n",
    "        project=config.project_name,\n",
    "        name=config.experiment_name,\n",
    "        exist_ok=True,\n",
    "        pretrained=True,\n",
    "        optimizer=config.optimizer,\n",
    "        verbose=config.verbose,\n",
    "        seed=42,\n",
    "        deterministic=True,\n",
    "        single_cls=config.single_cls,\n",
    "        amp=config.amp,\n",
    "        \n",
    "        # Learning rate settings\n",
    "        lr0=config.lr0,\n",
    "        lrf=config.lrf,\n",
    "        momentum=config.momentum,\n",
    "        weight_decay=config.weight_decay,\n",
    "        \n",
    "        # Loss weights - CRITICAL FOR SUBTLE DETECTION\n",
    "        cls=config.cls_loss_weight,\n",
    "        box=config.box_loss_weight,\n",
    "        dfl=config.dfl_loss_weight,\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        conf=config.conf_threshold,\n",
    "        iou=config.iou_threshold,\n",
    "        \n",
    "        # Enhanced augmentation\n",
    "        hsv_h=config.hsv_h,\n",
    "        hsv_s=config.hsv_s,\n",
    "        hsv_v=config.hsv_v,\n",
    "        degrees=config.degrees,\n",
    "        translate=config.translate,\n",
    "        scale=config.scale,\n",
    "        fliplr=config.fliplr,\n",
    "        flipud=config.flipud,\n",
    "        perspective=config.perspective,\n",
    "        mixup=config.mixup,\n",
    "        copy_paste=config.copy_paste,\n",
    "        \n",
    "        # Training schedule\n",
    "        warmup_epochs=5.0,\n",
    "        warmup_momentum=0.8,\n",
    "        warmup_bias_lr=0.1,\n",
    "        \n",
    "        # Validation settings\n",
    "        val=True,\n",
    "        plots=True,\n",
    "        save_json=True,\n",
    "        \n",
    "        # Additional settings for subtle detection\n",
    "        mosaic=0.8,  # Reduced mosaic augmentation\n",
    "        close_mosaic=15,  # Close mosaic earlier\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Enhanced training completed!\")\n",
    "    return results, model\n",
    "\n",
    "# Start training (this will take a while!)\n",
    "if dataset_valid:\n",
    "    print(\"Starting training... This may take 30+ minutes depending on your GPU.\")\n",
    "    training_results, trained_model = train_yolo_model(config)\n",
    "else:\n",
    "    print(\"❌ Cannot start training due to dataset issues. Please fix the dataset structure first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5aec4-edec-436f-af96-7e0b8c7c4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training report\n",
    "\n",
    "def analyze_training_results(project_name, experiment_name):\n",
    "    \"\"\"Analyze and visualize training results\"\"\"\n",
    "    \n",
    "    results_dir = Path(project_name) / experiment_name\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        print(f\"❌ Results directory not found: {results_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 Analyzing results from: {results_dir}\")\n",
    "    \n",
    "    # Display training curves\n",
    "    results_img = results_dir / 'results.png'\n",
    "    if results_img.exists():\n",
    "        print(\"=== Training Curves ===\")\n",
    "        display(Image(str(results_img)))\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    confusion_matrix_img = results_dir / 'confusion_matrix.png'\n",
    "    if confusion_matrix_img.exists():\n",
    "        print(\"=== Confusion Matrix ===\")\n",
    "        display(Image(str(confusion_matrix_img)))\n",
    "    \n",
    "    # Display validation predictions\n",
    "    val_batch_imgs = list(results_dir.glob('val_batch*_pred.jpg'))\n",
    "    if val_batch_imgs:\n",
    "        print(\"=== Validation Predictions ===\")\n",
    "        for img_path in val_batch_imgs[:2]:  # Show first 2 batches\n",
    "            print(f\"Validation predictions: {img_path.name}\")\n",
    "            display(Image(str(img_path)))\n",
    "    \n",
    "    # Get best model path\n",
    "    best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "    if best_model_path.exists():\n",
    "        print(f\"✅ Best model saved at: {best_model_path}\")\n",
    "        return best_model_path\n",
    "    else:\n",
    "        print(\"❌ Best model not found!\")\n",
    "        return None\n",
    "\n",
    "# Analyze results\n",
    "if 'training_results' in locals():\n",
    "    best_model_path = analyze_training_results(config.project_name, config.experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e39d1-41b0-4743-9a66-6198663685ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Validation and Metrics\n",
    "\n",
    "def validate_model(model_path):\n",
    "    \"\"\"Validate the trained model and show metrics\"\"\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🔍 Validating model...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Run validation\n",
    "    metrics = model.val()\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"=== Validation Metrics ===\")\n",
    "    print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "    print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "    print(f\"Recall: {metrics.box.mr:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Validate model\n",
    "if 'best_model_path' in locals() and best_model_path:\n",
    "    validation_metrics = validate_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbc8e1-7733-4a59-aa9f-f4bdd3023bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX export\n",
    "\n",
    "def export_to_onnx(model_path, output_name=\"stain_detection_model\"):\n",
    "    \"\"\"Export the trained model to ONNX format\"\"\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"📦 Exporting model to ONNX format...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    onnx_path = model.export(\n",
    "        format='onnx',\n",
    "        imgsz=640,\n",
    "        optimize=True,\n",
    "        half=False,  # Use FP32 for better compatibility\n",
    "        int8=False,\n",
    "        dynamic=False,\n",
    "        simplify=True,\n",
    "        opset=11,  # ONNX opset 11 for wide compatibility\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ ONNX model exported to: {onnx_path}\")\n",
    "    print(f\"Model size: {os.path.getsize(onnx_path) / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    return onnx_path\n",
    "\n",
    "# Export to ONNX\n",
    "if 'best_model_path' in locals() and best_model_path:\n",
    "    onnx_model_path = export_to_onnx(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4635828-5ede-4cf6-8260-4881789e54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "\n",
    "import random\n",
    "\n",
    "def get_grid_position(bbox, img_width, img_height):\n",
    "    \"\"\"Convert bounding box center to 3x3 grid position\"\"\"\n",
    "    # Get center of bounding box\n",
    "    center_x = bbox[0] + bbox[2] / 2\n",
    "    center_y = bbox[1] + bbox[3] / 2\n",
    "    \n",
    "    # Convert to grid coordinates (0-2 for both x and y)\n",
    "    grid_x = int(center_x / img_width * 3)\n",
    "    grid_y = int(center_y / img_height * 3)\n",
    "    \n",
    "    # Ensure within bounds\n",
    "    grid_x = max(0, min(2, grid_x))\n",
    "    grid_y = max(0, min(2, grid_y))\n",
    "    \n",
    "    # Convert to single grid position (0-8)\n",
    "    grid_position = grid_y * 3 + grid_x\n",
    "    \n",
    "    return grid_position\n",
    "\n",
    "def test_inference(model_path, test_image_path=None, confidence_threshold=0.5):\n",
    "    \"\"\"Test inference on a sample image\"\"\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    # Find a test image if not provided\n",
    "    if test_image_path is None:\n",
    "        test_dirs = ['dataset/test/images', '../test/images', 'val/images', '../val/images', 'valid/images', '../valid/images']\n",
    "        for test_dir in test_dirs:\n",
    "            if os.path.exists(test_dir):\n",
    "                test_images = [f for f in os.listdir(test_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                if test_images:\n",
    "                    test_image_path = os.path.join(test_dir, random.choice(test_images))\n",
    "                    break\n",
    "    \n",
    "    if test_image_path is None or not os.path.exists(test_image_path):\n",
    "        print(\"❌ No test image found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🧪 Testing inference on: {test_image_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Load and display original image\n",
    "    image = cv2.imread(test_image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_height, img_width = image_rgb.shape[:2]\n",
    "    \n",
    "    # Run prediction\n",
    "    results = model.predict(test_image_path, conf=confidence_threshold, verbose=False)\n",
    "    \n",
    "    # Process results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(image_rgb)\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Prediction results\n",
    "    ax2.imshow(image_rgb)\n",
    "    ax2.set_title('Predictions with Grid')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Draw grid lines\n",
    "    for i in range(1, 3):\n",
    "        ax2.axvline(x=img_width * i / 3, color='blue', linestyle='--', alpha=0.5)\n",
    "        ax2.axhline(y=img_height * i / 3, color='blue', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    stain_detected = False\n",
    "    detections = []\n",
    "    \n",
    "    # Process detections\n",
    "    for result in results:\n",
    "        if result.boxes is not None and len(result.boxes) > 0:\n",
    "            stain_detected = True\n",
    "            \n",
    "            for box in result.boxes:\n",
    "                # Get bounding box coordinates\n",
    "                bbox = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]\n",
    "                confidence = box.conf[0].cpu().numpy()\n",
    "                \n",
    "                # Draw bounding box\n",
    "                from matplotlib.patches import Rectangle\n",
    "                rect = Rectangle((bbox[0], bbox[1]), bbox[2]-bbox[0], bbox[3]-bbox[1], \n",
    "                               linewidth=2, edgecolor='red', facecolor='none')\n",
    "                ax2.add_patch(rect)\n",
    "                \n",
    "                # Get grid position\n",
    "                grid_pos = get_grid_position(bbox, img_width, img_height)\n",
    "                \n",
    "                # Add text annotation\n",
    "                ax2.text(bbox[0], bbox[1]-10, f'Stain: {confidence:.2f}\\\\nGrid: {grid_pos}', \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7),\n",
    "                        fontsize=10)\n",
    "                \n",
    "                detections.append({\n",
    "                    'grid_position': grid_pos,\n",
    "                    'confidence': float(confidence),\n",
    "                    'bbox': bbox.tolist()\n",
    "                })\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Stain detected: {stain_detected}\")\n",
    "    if stain_detected:\n",
    "        grid_map = {\n",
    "            0: \"Top-Left\", 1: \"Top-Center\", 2: \"Top-Right\",\n",
    "            3: \"Middle-Left\", 4: \"Middle-Center\", 5: \"Middle-Right\",\n",
    "            6: \"Bottom-Left\", 7: \"Bottom-Center\", 8: \"Bottom-Right\"\n",
    "        }\n",
    "        \n",
    "        for i, detection in enumerate(detections):\n",
    "            grid_pos = detection['grid_position']\n",
    "            confidence = detection['confidence']\n",
    "            print(f\"Detection {i+1}: Grid position {grid_pos} ({grid_map[grid_pos]}), Confidence: {confidence:.3f}\")\n",
    "    \n",
    "    return detections\n",
    "\n",
    "# Test inference\n",
    "if 'best_model_path' in locals() and best_model_path:\n",
    "    test_results = test_inference(best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
